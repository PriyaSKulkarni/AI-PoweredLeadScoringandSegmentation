{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR96xrmoeG-J",
        "outputId": "0d489681-4347-47bb-8cd0-90e3942340db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion rate by cluster on training data:\n",
            "cluster\n",
            "0    0.500864\n",
            "1    0.000000\n",
            "2    0.000000\n",
            "Name: converted, dtype: float64\n",
            "\n",
            "Cluster with highest conversion rate: 0 (50.09%)\n",
            "Cluster descriptions saved to cluster_descriptions.json\n",
            "\n",
            "Key characteristics of cluster 0 with highest conversion rate:\n",
            "higher contacted, higher demo_given, lower employee_count, higher annual_revenue, higher industry_encoded\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('synthetic_leads_dataset_large.csv')\n",
        "\n",
        "# Encode categorical feature\n",
        "le = LabelEncoder()\n",
        "df['industry_encoded'] = le.fit_transform(df['industry'])\n",
        "\n",
        "# Define features and target\n",
        "features = [\"contacted\", \"demo_given\", \"employee_count\", \"annual_revenue\", \"industry_encoded\"]\n",
        "X = df[features]\n",
        "y = df['converted']\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Train KMeans on entire scaled dataset\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Analyze clusters on training data\n",
        "import numpy as np\n",
        "\n",
        "# Convert X_train back to DataFrame for analysis\n",
        "X_train_df = pd.DataFrame(X_train, columns=features)\n",
        "\n",
        "# Predict clusters on training data\n",
        "train_clusters = kmeans.predict(X_train)\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "train_analysis = X_train_df.copy()\n",
        "train_analysis['cluster'] = train_clusters\n",
        "train_analysis['converted'] = y_train.values\n",
        "\n",
        "# Calculate conversion rate per cluster\n",
        "cluster_conversion_rates = train_analysis.groupby('cluster')['converted'].mean()\n",
        "print(\"Conversion rate by cluster on training data:\")\n",
        "print(cluster_conversion_rates)\n",
        "\n",
        "# Identify cluster with highest conversion rate\n",
        "best_cluster = cluster_conversion_rates.idxmax()\n",
        "best_conversion_rate = cluster_conversion_rates.max()\n",
        "print(f\"\\nCluster with highest conversion rate: {best_cluster} ({best_conversion_rate:.2%})\")\n",
        "\n",
        "# Analyze feature means for best cluster vs overall mean\n",
        "overall_mean = X_train_df.mean()\n",
        "best_cluster_mean = train_analysis[train_analysis['cluster'] == best_cluster][features].mean()\n",
        "\n",
        "# Find significant feature differences (10% threshold)\n",
        "threshold = 0.1 * overall_mean.abs()\n",
        "\n",
        "feature_diffs = []\n",
        "for feat in features:\n",
        "    diff = best_cluster_mean[feat] - overall_mean[feat]\n",
        "    if abs(diff) > threshold[feat]:\n",
        "        direction = \"higher\" if diff > 0 else \"lower\"\n",
        "        feature_diffs.append(f\"{direction} {feat}\")\n",
        "\n",
        "import json\n",
        "\n",
        "cluster_descriptions = {}\n",
        "\n",
        "for cluster_num in range(kmeans.n_clusters):\n",
        "    cluster_data = train_analysis[train_analysis['cluster'] == cluster_num]\n",
        "    conv_rate = cluster_data['converted'].mean()\n",
        "    feature_means = cluster_data[features].mean()\n",
        "    overall_means = train_analysis[features].mean()\n",
        "    threshold = 0.1 * overall_means.abs()\n",
        "\n",
        "    diffs = []\n",
        "    for feat in features:\n",
        "        diff = feature_means[feat] - overall_means[feat]\n",
        "        if abs(diff) > threshold[feat]:\n",
        "            direction = \"higher\" if diff > 0 else \"lower\"\n",
        "            diffs.append(f\"{direction} {feat}\")\n",
        "\n",
        "    desc = f\"Conversion rate: {conv_rate:.2%}. Leads tend to have \" + \", \".join(diffs) if diffs else \"typical feature values.\"\n",
        "    cluster_descriptions[cluster_num] = desc\n",
        "\n",
        "with open(\"cluster_descriptions.json\", \"w\") as f:\n",
        "    json.dump(cluster_descriptions, f)\n",
        "\n",
        "print(\"Cluster descriptions saved to cluster_descriptions.json\")\n",
        "print(f\"\\nKey characteristics of cluster {best_cluster} with highest conversion rate:\")\n",
        "print(\", \".join(feature_diffs) if feature_diffs else \"No significant feature differences found.\")\n",
        "\n",
        "# Save models and test set for later use\n",
        "joblib.dump(rf, 'model.pkl')\n",
        "joblib.dump(kmeans, 'kmeans.pkl')\n",
        "pd.DataFrame(X_test, columns=features).to_csv('X_test.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRygJyTfeO1j",
        "outputId": "66b7257d-31c8-4fce-c5f6-fde5e5246f7e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.47.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.47.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.2.12 streamlit-1.47.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "\n",
        "st.set_page_config(page_title=\"Lead Scoring Dashboard\", layout=\"wide\")\n",
        "\n",
        "# Load models and data\n",
        "model = joblib.load(\"model.pkl\")\n",
        "kmeans = joblib.load(\"kmeans.pkl\")\n",
        "X = pd.read_csv(\"X_test.csv\")\n",
        "\n",
        "# Predict clusters\n",
        "cluster_labels = kmeans.predict(X)\n",
        "df_with_clusters = X.copy()\n",
        "df_with_clusters['cluster'] = cluster_labels\n",
        "\n",
        "# Title\n",
        "st.title(\"🔍 AI-Powered Lead Scoring Dashboard\")\n",
        "\n",
        "# Sidebar\n",
        "lead_idx = st.sidebar.selectbox(\"Select Lead Index\", range(len(X)))\n",
        "lead = X.iloc[lead_idx:lead_idx+1]\n",
        "lead_cluster = cluster_labels[lead_idx]\n",
        "prediction = model.predict_proba(lead)[0][1]\n",
        "\n",
        "st.sidebar.markdown(f\"\"\"\n",
        "**Prediction:** `{prediction:.2f}`\n",
        "**Cluster:** `Cluster {lead_cluster}`\n",
        "\"\"\")\n",
        "\n",
        "# Main area\n",
        "st.subheader(f\"📊 Lead #{lead_idx} Prediction Summary\")\n",
        "st.markdown(f\"\"\"\n",
        "- 🎯 **Predicted Conversion Probability:** `{prediction:.2f}`\n",
        "- 🧬 **Assigned Cluster:** `Cluster {lead_cluster}`\n",
        "\"\"\")\n",
        "\n",
        "# Lead feature values\n",
        "st.subheader(\"🔎 Lead Feature Values\")\n",
        "st.dataframe(lead.T, use_container_width=True)\n",
        "\n",
        "# Cluster summary\n",
        "st.subheader(\"🧠 Cluster Analysis\")\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    st.markdown(\"**🔹 Mean Feature Values by Cluster**\")\n",
        "    st.dataframe(df_with_clusters.groupby(\"cluster\").mean().round(2))\n",
        "\n",
        "with col2:\n",
        "    st.markdown(\"**🔸 Cluster Distribution**\")\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.countplot(x=\"cluster\", data=df_with_clusters, ax=ax)\n",
        "    ax.set_title(\"Number of Leads per Cluster\")\n",
        "    st.pyplot(fig)\n",
        "\n",
        "# Google GenAI (Gemini) for personalized message\n",
        "st.subheader(\"💡 Google GenAI \")\n",
        "use_genai = st.checkbox(\"Generate a personalized message for this lead using GenAI?\")\n",
        "import google.generativeai as genai\n",
        "\n",
        "with open(\"cluster_descriptions.json\", \"r\") as f:\n",
        "    cluster_descriptions = json.load(f)\n",
        "\n",
        "if use_genai:\n",
        "    # Replace with your actual key\n",
        "    genai.configure(api_key=\"AIzaSyBiwkASZfqswK5Mon86kOpt9MuSUft9las\")\n",
        "    model_genai = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
        "\n",
        "    cluster_desc = cluster_descriptions.get(str(lead_cluster), \"No description available.\")\n",
        "\n",
        "# Create your prompt incorporating the cluster description\n",
        "    prompt = f\"\"\"\n",
        "    You are a marketing expert. This lead belongs to cluster {lead_cluster} with the following characteristics:\n",
        "    {cluster_desc}\n",
        "    The predicted conversion probability for this lead is {prediction:.2f}.\n",
        "    Generate a short, compelling personalized message to engage this lead.\n",
        "    \"\"\"\n",
        "\n",
        "    response = model_genai.generate_content(f\"generate a short, compelling personalized message to engage this lead:\\n{prompt}\")\n",
        "\n",
        "    st.markdown(\"**📢 Suggested Message:**\")\n",
        "    st.success(response.text if hasattr(response, 'text') else \"No response.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnUwAmu1eUkO",
        "outputId": "6ec445c6-b3a1-4e68-ff9a-4af10fb9dc63"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /root/.config/ngrok/ngrok.yml\n",
        "from pyngrok import ngrok\n",
        "!ngrok authtoken '30RJjlYWNfx0mkfwxuJ8wXNlXvU_5XxfD5fQ7V969PK6tY8FD'\n",
        "\n",
        "# Kill existing tunnels (if any)\n",
        "ngrok.kill()\n",
        "\n",
        "# Run Streamlit app in background\n",
        "\n",
        "get_ipython().system_raw('streamlit run app.py &')\n",
        "\n",
        "# Open tunnel\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(f\"Streamlit app URL: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zl4afJBeXc9",
        "outputId": "cd8526e5-566a-4b5b-fba3-27291cc14e7f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Streamlit app URL: NgrokTunnel: \"https://fef36c808b28.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xm53iVV8eY-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}